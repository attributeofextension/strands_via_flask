### Flask Web Server with Strands Agentic AI framework
Takes a single prompt and returns a response from an agent

You'll need to install ollama on your machine and pull the llama3 model.

Once you've cloned the repo, run `docker-compose up --build`.

Finally, to get a response to a prompt, navigate to localhost:8000/?prompt=<content+of+prompt> (use a plus instead of spacebar).